# ========================================
# ElizaOS Rust - Environment Configuration
# ========================================
# 
# SETUP INSTRUCTIONS:
# 1. Copy this file: cp .env.example .env
# 2. Generate secure keys (see Security Configuration below)
# 3. Add your API keys
# 4. Secure the file: chmod 600 .env

# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_MODEL=gpt-4
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=2000

# Anthropic (Claude) Configuration
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-opus-20240229
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_MAX_TOKENS=2000

# Local LLM Configuration (privacy-first, no API keys needed)
LOCAL_LLM_BACKEND=ollama
LOCAL_LLM_ENDPOINT=http://localhost:11434
LOCAL_LLM_MODEL=phi3:mini
LOCAL_LLM_TEMPERATURE=0.7
LOCAL_LLM_MAX_TOKENS=2000

# Database Configuration
DATABASE_URL=sqlite:eliza.db
# For PostgreSQL (production):
# DATABASE_URL=postgresql://eliza:password@localhost:5432/eliza?sslmode=require

# Security Configuration
# IMPORTANT: Generate your own random keys with:
#   openssl rand -base64 32  # for ENCRYPTION_KEY
#   openssl rand -base64 24  # for SECRET_SALT
ENCRYPTION_KEY=GENERATE_YOUR_OWN_KEY_DO_NOT_USE_THIS_EXAMPLE
SECRET_SALT=GENERATE_YOUR_OWN_SALT_DO_NOT_USE_THIS

# Server Configuration
SERVER_HOST=127.0.0.1
SERVER_PORT=3000
ENABLE_CORS=true

# Logging Configuration
RUST_LOG=info,eliza_core=debug
LOG_LEVEL=info

# Rate Limiting
RATE_LIMIT_WINDOW_SECONDS=60
RATE_LIMIT_MAX_REQUESTS=100

# Distributed Runtime (optional - for multi-node deployment)
NODE_ID=
NODE_NAME=primary
NODE_ADDRESS=
CLUSTER_HEARTBEAT_INTERVAL=30

# Feature Flags
ENABLE_DISTRIBUTED=false
ENABLE_STREAMING=true
ENABLE_EMBEDDINGS=true
STRICT_COMPLIANCE_MODE=false

# Compliance Settings
# Options for COMPLIANCE_MODE: standard, hipaa, government
COMPLIANCE_MODE=standard
DATA_RETENTION_DAYS=90
AUDIT_LOGGING=false

# Multi-Agent Configuration (optional)
ENABLE_MULTI_AGENT=false
COORDINATOR_ENDPOINT=

# Redis Configuration (optional - for caching/queues)
# REDIS_URL=redis://localhost:6379

# MCP (Model Context Protocol) Server Configuration
# SECURITY NOTE: MCP server is DISABLED by default for security
# Only enable if you need remote model training capabilities
MCP_ENABLED=false
MCP_HOST=127.0.0.1              # Localhost only by default (use 0.0.0.0 for remote access)
MCP_PORT=3000

# MCP Authentication
# IMPORTANT: Required if MCP_REQUIRE_AUTH=true
# Generate secure token with: openssl rand -base64 32
MCP_REQUIRE_AUTH=true
MCP_AUTH_TOKEN=GENERATE_YOUR_OWN_TOKEN_DO_NOT_USE_THIS

# MCP Rate Limiting
MCP_ENABLE_RATE_LIMIT=true
MCP_RATE_LIMIT_RPM=60           # Requests per minute per token

# MCP TLS/SSL (optional, recommended for production)
MCP_USE_TLS=false
# MCP_TLS_CERT=/path/to/cert.pem
# MCP_TLS_KEY=/path/to/key.pem

# MCP CORS (optional, for web clients)
MCP_ENABLE_CORS=false
# MCP_CORS_ORIGINS=https://example.com,https://app.example.com

# Additional Service Keys (add as needed)
# CUSTOM_SERVICE_API_KEY=
# CUSTOM_SERVICE_ENDPOINT=

# ========================================
# Local Infrastructure Configuration
# ========================================

# Local Vector Database (no PostgreSQL required)
# Uses HNSW for fast similarity search
LOCAL_VECTOR_ENABLED=true
LOCAL_VECTOR_DATA_DIR=./data/vectors
LOCAL_VECTOR_DIMENSION=1536          # OpenAI embedding dimension
LOCAL_VECTOR_MAX_ELEMENTS=100000
LOCAL_VECTOR_HNSW_M=16              # HNSW M parameter (bi-directional links)
LOCAL_VECTOR_HNSW_EF=200            # HNSW ef_construction parameter

# Hardware Detection & Optimization
# Automatically detects CPU, GPU, RAM and optimizes settings
HARDWARE_AUTO_DETECT=true
HARDWARE_OPTIMIZE_ON_START=true

# Model Routing Configuration
# Automatically routes tasks to optimal models based on hardware
MODEL_ROUTING_ENABLED=true
MODEL_ROUTING_PREFERENCE=balanced    # Options: speed, quality, balanced

# Hardware Constraints (auto-detected if not specified)
# HARDWARE_AVAILABLE_MEMORY_GB=8.0
# HARDWARE_HAS_GPU=false
# HARDWARE_MAX_CONTEXT_LENGTH=4096

# Model Recommendations
# Override auto-detected models for specific tasks
# MODEL_CHAT=phi3:mini
# MODEL_CODE_GENERATION=codellama:13b
# MODEL_SUMMARIZATION=llama3.2:3b
# MODEL_EMBEDDING=nomic-embed-text
